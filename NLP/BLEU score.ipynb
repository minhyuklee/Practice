{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f2239769b62f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Requirement already satisfied: click in c:\\users\\minhyuklee\\anaconda3\\envs\\research\\lib\\site-packages (from nltk) (7.1.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\minhyuklee\\anaconda3\\envs\\research\\lib\\site-packages (from nltk) (0.14.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\minhyuklee\\anaconda3\\envs\\research\\lib\\site-packages (from nltk) (4.43.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=daf1daad3f8df3df80d98166116d653d263616365b9924ec70c01f06f39b3d01\n",
      "  Stored in directory: c:\\users\\minhyuklee\\appdata\\local\\pip\\cache\\wheels\\45\\6c\\46\\a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.5 regex-2020.11.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_count(tokens, n): # tokens: 토큰화된 candidate 문장, n: n-gram의 n\n",
    "    return Counter(ngrams(tokens, n)) # 문장에서 n-gram을 카운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the',): 3, ('It',): 1, ('is',): 1, ('a',): 1, ('guide',): 1, ('to',): 1, ('action',): 1, ('which',): 1, ('ensures',): 1, ('that',): 1, ('military',): 1, ('always',): 1, ('obeys',): 1, ('commands',): 1, ('of',): 1, ('party.',): 1})\n"
     ]
    }
   ],
   "source": [
    "candidate = \"It is a guide to action which ensures that the military always obeys the commands of the party.\"\n",
    "tokens = candidate.split() # 단어 토큰화\n",
    "result = simple_count(tokens, 1) # 토큰화된 문장, 유니그램의 개수를 구하려면 n=1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the',): 7})\n"
     ]
    }
   ],
   "source": [
    "candidate = \"the the the the the the the\"\n",
    "tokens = candidate.split()\n",
    "result = simple_count(tokens, 1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clip(candidate, reference_list, n):\n",
    "    cnt_ca = simple_count(candidate, n)\n",
    "    # Ca 문장에서 n-gram 카운트\n",
    "    temp = dict() # simple_count의 출력의 데이터 타입이 dict라서\n",
    "    \n",
    "    for ref in reference_list: # 다수의 Ref 문장에 대해서 이하 반복\n",
    "        cnt_ref = simple_count(ref, n)\n",
    "        # Ref 문장에서 n-gram 카운트\n",
    "        \n",
    "        for n_gram in cnt_ref: # 모든 Ref에 대해서 비교하여 특정 n-gram이 하나의 Ref에 가장 많이 등장한 횟수를 저장\n",
    "            if n_gram in temp:\n",
    "                temp[n_gram] = max(cnt_ref[n_gram], temp[n_gram]) # max_ref_count\n",
    "            else:\n",
    "                temp[n_gram] = cnt_ref[n_gram]\n",
    "                \n",
    "    return {\n",
    "        n_gram: min(cnt_ca.get(n_gram, 0), temp.get(n_gram, 0)) for n_gram in cnt_ca\n",
    "        # count_clip=min(count, max_ref_count)\n",
    "        # 위의 get은 찾고자 하는 n-gram이 없으면 0을 반환.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('the',): 2}\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the'\n",
    "references = [\n",
    "    'the cat is on the mat',\n",
    "    'there is a cat on the mat'\n",
    "]\n",
    "result = count_clip(candidate.split(), list(map(lambda ref: ref.split(), references)), 1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(candidate, reference_list, n):\n",
    "    clip = count_clip(candidate, reference_list, n)\n",
    "    total_clip = sum(clip.values()) # 분자\n",
    "    \n",
    "    ct = simple_count(candidate, n)\n",
    "    total_ct = sum(ct.values()) # 분모\n",
    "    \n",
    "    if total_ct == 0: # n-gram의 n이 커졌을 때 분모가 0이 되는 것을 방지\n",
    "        total_ct = 1\n",
    "    \n",
    "    return (total_clip / total_ct) # 보정된 정밀도\n",
    "    # count_clip의 합을 분자로 하고 단순 count의 합을 분모로 하면 보정된 정밀도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "result = modified_precision(candidate.split(), list(map(lambda ref: ref.split(), references)), 1) # 유니그램이므로 n=1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_ref_length(candidate, reference_list): # Ca 길이와 가장 근접한 Ref의 길이를 리턴하는 함수\n",
    "    ca_len = len(candidate) # ca 길이\n",
    "    ref_lens = (len(ref) for ref in reference_list) # Ref들의 길이\n",
    "    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - ca_len), ref_len))\n",
    "    # 길이 차이를 최소화하는 Ref를 찾아서 Ref의 길이를 리턴\n",
    "    return closest_ref_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate, reference_list):\n",
    "    ca_len = len(candidate)\n",
    "    ref_len = closest_ref_length(candidate, reference_list)\n",
    "    \n",
    "    if ca_len > ref_len:\n",
    "        return 1\n",
    "    elif ca_len == 0:\n",
    "    # candidate가 비어있다면 BP=0 --> BLEU=0\n",
    "        return 0\n",
    "    else:\n",
    "        return np.exp(1 - ref_len/ca_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(candidate, reference_list, weights = [0.25, 0.25, 0.25, 0.25]):\n",
    "    bp = brevity_penalty(candidate, reference_list) # 브레버티 패널티, BP\n",
    "    \n",
    "    p_n = [modified_precision(candidate, reference_list, n=n) for n, _ in enumerate(weights, start=1)]\n",
    "    # p1, p2, p3, ... pn\n",
    "    score = np.sum([w_i * np.log(p_i) if p_i != 0 else 0 for w_i, p_i in zip(weights, p_n)])\n",
    "    return bp * np.exp(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5045666840058485\n",
      "0.5045666840058485\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "candidate = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "references = [\n",
    "    'It is a guide to action that ensures that the military will forever heed Party commands',\n",
    "    'It is the guiding principle which guarantees the military forces always being under the command of the Party',\n",
    "    'It is the practical guide for the army always to heed the directions of the party'    \n",
    "]\n",
    "\n",
    "print(bleu_score(candidate.split(), list(map(lambda ref: ref.split(), references))))\n",
    "print(bleu.sentence_bleu(list(map(lambda ref: ref.split(), references)),candidate.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
