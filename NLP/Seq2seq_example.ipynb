{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib3\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "    \n",
    "http = urllib3.PoolManager()\n",
    "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path,filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
    "    shutil.copyfileobj(r, out_file)\n",
    "    \n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179904"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21573</th>\n",
       "      <td>She is attractive.</td>\n",
       "      <td>Elle est attirante.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32385</th>\n",
       "      <td>Put some clothes on.</td>\n",
       "      <td>Habillez-vous !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43153</th>\n",
       "      <td>I guess the dog bites.</td>\n",
       "      <td>Je crois que le chien mord.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43090</th>\n",
       "      <td>I find you attractive.</td>\n",
       "      <td>Je vous trouve attirantes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33851</th>\n",
       "      <td>We'll never do that.</td>\n",
       "      <td>Nous ne ferons jamais ça.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40170</th>\n",
       "      <td>We're getting closer.</td>\n",
       "      <td>Nous nous approchons.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33562</th>\n",
       "      <td>Tom was a great guy.</td>\n",
       "      <td>Tom était un mec génial.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51952</th>\n",
       "      <td>Stop talking like that.</td>\n",
       "      <td>Arrêtez de parler comme ça.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41795</th>\n",
       "      <td>Does Tom have a beard?</td>\n",
       "      <td>Tom a-t-il une barbe ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54900</th>\n",
       "      <td>Did I hurt his feelings?</td>\n",
       "      <td>L'ai-je froissé ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                          tar\n",
       "21573        She is attractive.          Elle est attirante.\n",
       "32385      Put some clothes on.              Habillez-vous !\n",
       "43153    I guess the dog bites.  Je crois que le chien mord.\n",
       "43090    I find you attractive.   Je vous trouve attirantes.\n",
       "33851      We'll never do that.    Nous ne ferons jamais ça.\n",
       "40170     We're getting closer.        Nous nous approchons.\n",
       "33562      Tom was a great guy.     Tom était un mec génial.\n",
       "51952   Stop talking like that.  Arrêtez de parler comme ça.\n",
       "41795    Does Tom have a beard?       Tom a-t-il une barbe ?\n",
       "54900  Did I hurt his feelings?            L'ai-je froissé ?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:60000] # 6만개만 저장\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>Ask anybody.</td>\n",
       "      <td>\\t Demande à qui que ce soit ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22042</th>\n",
       "      <td>They are very big.</td>\n",
       "      <td>\\t Elles sont très grandes. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21708</th>\n",
       "      <td>Stop overreacting.</td>\n",
       "      <td>\\t Cessez de réagir de façon excessive. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24386</th>\n",
       "      <td>Go and wake her up.</td>\n",
       "      <td>\\t Va la réveiller. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21538</th>\n",
       "      <td>She decided to go.</td>\n",
       "      <td>\\t Elle a décidé de partir. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46854</th>\n",
       "      <td>What do you call this?</td>\n",
       "      <td>\\t Comment appelles-tu ceci ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6350</th>\n",
       "      <td>I'm your boss.</td>\n",
       "      <td>\\t Je suis ton chef. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36632</th>\n",
       "      <td>I hated you at first.</td>\n",
       "      <td>\\t Je vous détestais, au début. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18524</th>\n",
       "      <td>You're assertive.</td>\n",
       "      <td>\\t Tu sais te faire comprendre. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Kiss me.</td>\n",
       "      <td>\\t Embrassez-moi. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                         tar\n",
       "2018             Ask anybody.           \\t Demande à qui que ce soit ! \\n\n",
       "22042      They are very big.              \\t Elles sont très grandes. \\n\n",
       "21708      Stop overreacting.  \\t Cessez de réagir de façon excessive. \\n\n",
       "24386     Go and wake her up.                      \\t Va la réveiller. \\n\n",
       "21538      She decided to go.              \\t Elle a décidé de partir. \\n\n",
       "46854  What do you call this?            \\t Comment appelles-tu ceci ? \\n\n",
       "6350           I'm your boss.                     \\t Je suis ton chef. \\n\n",
       "36632   I hated you at first.          \\t Je vous détestais, au début. \\n\n",
       "18524       You're assertive.          \\t Tu sais te faire comprendre. \\n\n",
       "206                  Kiss me.                        \\t Embrassez-moi. \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 집합 구축\n",
    "src_vocab=set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 글자씩 읽음\n",
    "        if char not in src_vocab:\n",
    "            src_vocab.add(char)\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        if char not in tar_vocab:\n",
    "            tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)\n",
    "tar_vocab_size = len(tar_vocab)\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g': 0, '%': 1, 'F': 2, 'S': 3, 'x': 4, 'L': 5, '!': 6, '5': 7, '1': 8, 'R': 9, ':': 10, 'B': 11, 'b': 12, 'r': 13, 'D': 14, '-': 15, 'h': 16, 'K': 17, 'j': 18, '\"': 19, '6': 20, 'z': 21, '.': 22, 'T': 23, 'X': 24, ',': 25, 'c': 26, 'a': 27, 'O': 28, '4': 29, 'k': 30, 'd': 31, 'A': 32, 'i': 33, 'u': 34, 'n': 35, 'y': 36, '8': 37, 'U': 38, '€': 39, 'V': 40, 'e': 41, ' ': 42, 'Q': 43, '’': 44, 'E': 45, '3': 46, 'o': 47, '0': 48, \"'\": 49, 'W': 50, 'q': 51, '$': 52, 'Z': 53, 'm': 54, '7': 55, 'é': 56, 'P': 57, 'w': 58, 'N': 59, 's': 60, '2': 61, 'I': 62, '/': 63, 'M': 64, 'v': 65, '&': 66, 'G': 67, 'C': 68, '9': 69, 'Y': 70, 't': 71, 'J': 72, '?': 73, 'l': 74, 'f': 75, 'H': 76, 'p': 77}\n",
      "{'g': 0, '%': 1, 'F': 2, 'S': 3, 'x': 4, '\\u200b': 5, '\\t': 6, 'L': 7, '!': 8, '\\u202f': 9, '5': 10, '(': 11, '1': 12, 'R': 13, 'û': 14, ':': 15, 'B': 16, 'b': 17, 'r': 18, '-': 19, 'D': 20, 'h': 21, 'K': 22, '\\n': 23, 'É': 24, 'Ç': 25, 'j': 26, '\"': 27, 'ë': 28, '6': 29, 'ï': 30, 'z': 31, '.': 32, 'T': 33, 'X': 34, ',': 35, 'c': 36, 'a': 37, 'O': 38, 'Ô': 39, '4': 40, 'k': 41, 'd': 42, 'A': 43, 'u': 44, 'i': 45, 'С': 46, '«': 47, 'n': 48, 'ê': 49, 'y': 50, 'è': 51, 'â': 52, '\\xa0': 53, '8': 54, 'U': 55, 'ù': 56, 'V': 57, ' ': 58, 'e': 59, 'Q': 60, '’': 61, '3': 62, 'E': 63, 'o': 64, '\\u2009': 65, 'ç': 66, 'î': 67, '0': 68, \"'\": 69, 'q': 70, '$': 71, 'W': 72, ')': 73, 'Z': 74, 'm': 75, 'é': 76, '7': 77, 'P': 78, 'w': 79, 'Ê': 80, 'N': 81, 'à': 82, 's': 83, '2': 84, '‘': 85, 'I': 86, 'M': 87, 'v': 88, '&': 89, 'G': 90, 'C': 91, '9': 92, 'œ': 93, '»': 94, 'Y': 95, 't': 96, 'J': 97, 'ô': 98, '?': 99, 'À': 100, 'l': 101, 'f': 102, 'H': 103, 'p': 104}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[67, 47, 22], [76, 33, 22], [76, 33, 22], [9, 34, 35, 6], [9, 34, 35, 6]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 58, 57, 37, 58, 8, 58, 23], [6, 58, 3, 37, 101, 44, 96, 58, 8, 58, 23], [6, 58, 3, 37, 101, 44, 96, 32, 58, 23], [6, 58, 91, 64, 44, 18, 83, 9, 8, 58, 23], [6, 58, 91, 64, 44, 18, 59, 31, 9, 8, 58, 23]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58, 57, 37, 58, 8, 58, 23], [58, 3, 37, 101, 44, 96, 58, 8, 58, 23], [58, 3, 37, 101, 44, 96, 32, 58, 23], [58, 91, 64, 44, 18, 83, 9, 8, 58, 23], [58, 91, 64, 44, 18, 59, 31, 9, 8, 58, 23]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t>0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태.\n",
    "\n",
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 19s 400us/sample - loss: 0.6455 - val_loss: 0.5898\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 14s 301us/sample - loss: 0.4042 - val_loss: 0.4824\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 15s 304us/sample - loss: 0.3400 - val_loss: 0.4323\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.3044 - val_loss: 0.4042\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.2808 - val_loss: 0.3872\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 15s 304us/sample - loss: 0.2637 - val_loss: 0.3742\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.2502 - val_loss: 0.3684\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.2390 - val_loss: 0.3626\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.2296 - val_loss: 0.3571\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.2212 - val_loss: 0.3568\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 15s 312us/sample - loss: 0.2139 - val_loss: 0.3560\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 15s 315us/sample - loss: 0.2074 - val_loss: 0.3532\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 15s 312us/sample - loss: 0.2014 - val_loss: 0.3540\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.1960 - val_loss: 0.3545\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.1910 - val_loss: 0.3563\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.1864 - val_loss: 0.3549\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1821 - val_loss: 0.3594\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.1781 - val_loss: 0.3609\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.1744 - val_loss: 0.3623\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.1709 - val_loss: 0.3643\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.1676 - val_loss: 0.3668\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.1646 - val_loss: 0.3683\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.1616 - val_loss: 0.3720\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1589 - val_loss: 0.3734\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.1562 - val_loss: 0.3763\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.1539 - val_loss: 0.3794\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1517 - val_loss: 0.3819\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.1494 - val_loss: 0.3822\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.1474 - val_loss: 0.3866\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1453 - val_loss: 0.3879\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.1433 - val_loss: 0.3916\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1416 - val_loss: 0.3948\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1399 - val_loss: 0.3973\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.1382 - val_loss: 0.4026\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.1367 - val_loss: 0.4013\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 15s 311us/sample - loss: 0.1352 - val_loss: 0.4035\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.1337 - val_loss: 0.4057\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1321 - val_loss: 0.4109\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.1308 - val_loss: 0.4126\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.1295 - val_loss: 0.4145\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.1283 - val_loss: 0.4170\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.1268 - val_loss: 0.4217\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.1258 - val_loss: 0.4230\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 15s 304us/sample - loss: 0.1248 - val_loss: 0.4243\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1237 - val_loss: 0.4264\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1226 - val_loss: 0.4302\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1217 - val_loss: 0.4306\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 15s 304us/sample - loss: 0.1207 - val_loss: 0.4340\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.1197 - val_loss: 0.4340\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 15s 311us/sample - loss: 0.1188 - val_loss: 0.4382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2426631a188>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=32, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2seq 기계 번역기 동작시키기\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현.\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM이 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "\n",
    "# index로부터 단어를 얻을 수 있는 변수\n",
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1. # \\t = 6, \\n = 23, \\t 위치의 값을 1. 으로 바꿔주어 원-핫 벡터를 만든다는 의미.\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        \n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "            \n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  Consez ! \n",
      "-----------------------------------\n",
      "입력 문장: I left.\n",
      "정답 문장:  Je suis parti. \n",
      "번역기가 번역한 문장:  J'ai compris. \n",
      "-----------------------------------\n",
      "입력 문장: Burn it.\n",
      "정답 문장:  Brûlez-la. \n",
      "번역기가 번역한 문장:  Brûle-le. \n",
      "-----------------------------------\n",
      "입력 문장: Drive on.\n",
      "정답 문장:  Continue à rouler ! \n",
      "번역기가 번역한 문장:  Avancez ! \n",
      "-----------------------------------\n",
      "입력 문장: Step back.\n",
      "정답 문장:  Recule ! \n",
      "번역기가 번역한 문장:  Reculez! Nous finier ! \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # 앞뒤로 \\t와 \\n 제거하고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # 마지막에 \\n 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
